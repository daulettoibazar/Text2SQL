{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7eea1eae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 13 09:13:56 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    57W / 300W |   1427MiB / 32768MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     32784      C   ...azd/miniconda3/bin/python     1424MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5881ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading module for CUDA 11.4.4\r",
      "\r\n",
      "CUDA 11.4.4 is now loaded\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!module load cuda/11.4.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c55695",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e45abbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5f49ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 1684 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e41aed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82c105a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-13 09:14:12,702] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 09:14:19.580805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 09:14:19.865448: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-13 09:14:25.346993: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-13 09:14:25.347216: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-13 09:14:25.347245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"t5-small\"\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d0d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab4ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c10ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7083684",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('Training_set_IMDB/training_set_sample_no_target.csv')\n",
    "df1 = df1.dropna()\n",
    "\n",
    "\n",
    "shuffled_df = df1.sample(frac=1, random_state=42)  # Set random_state for reproducibility\n",
    "\n",
    "train_df, eval_df = train_test_split(shuffled_df, test_size=0.1, random_state=42)\n",
    "\n",
    "# 'train_df' will contain 80% of the shuffled data for training\n",
    "# 'eval_df' will contain 20% of the shuffled data for evaluation\n",
    "\n",
    "\n",
    "text = []\n",
    "sql = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    \n",
    "    text_input = \"Translate to SQL: \" + row['Text'],\n",
    "    sql_input = row['SQL']\n",
    "    text.append(text_input)\n",
    "    sql.append(sql_input)\n",
    "    \n",
    "inputs = {\"inputs\": text,\n",
    "        \"target\": sql}\n",
    "\n",
    "train_dataset = Dataset.from_dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc786cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = []\n",
    "sql_2 = []\n",
    "\n",
    "for index, row in eval_df.iterrows():\n",
    "    \n",
    "    text_input = \"Translate to SQL: \" + row['Text'],\n",
    "    sql_input = row['SQL']\n",
    "    text_2.append(text_input)\n",
    "    sql_2.append(sql_input)\n",
    "    \n",
    "inputs_2 = {\"inputs\": text_2,\n",
    "        \"target\": sql_2}\n",
    "eval_dataset = Dataset.from_dict(inputs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e24fc70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_to_lenght(x):\n",
    "    x[\"input_len\"] = len(tokenizer(x[\"inputs\"]).input_ids)\n",
    "    x[\"input_longer_256\"] = int(x[\"input_len\"]>256)\n",
    "    x[\"input_longer_128\"] = int(x[\"input_len\"]>128)\n",
    "    x[\"input_longet_64\"] = int(x[\"input_len\"]>64)\n",
    "    x[\"output_len\"] = len(tokenizer(x[\"target\"]).input_ids)\n",
    "    x[\"output_longet_256\"] = int(x[\"output_len\"]>256)\n",
    "    x[\"output_longet_128\"] = int(x[\"output_len\"]>128)\n",
    "    x[\"output_longet_64\"] = int(x[\"output_len\"]>64)\n",
    "    return x\n",
    "\n",
    "sample_size = 10000\n",
    "data_stats = train_dataset.select(range(sample_size)).map(map_to_lenght, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62d76548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean: 1.0 \n",
      " % of input len > 256: 0.0, \n",
      " % of input len > 128: 0.0, \n",
      " % of input len > 64: 0.0, \n",
      " Ouput mean: 106.4648,\n",
      "% of output len > 256: 0.0, \n",
      "% of output len > 128: 0.2934, \n",
      "% of output len > 64: 0.7241\n"
     ]
    }
   ],
   "source": [
    "def compute_and_print(x):\n",
    "    if len(x[\"input_len\"])==sample_size:\n",
    "        print(\n",
    "            f\"Input mean: {sum(x['input_len'])/sample_size} \\n % of input len > 256: {sum(x['input_longer_256'])/sample_size}, \\n % of input len > 128: {sum(x['input_longer_128'])/sample_size}, \\n % of input len > 64: {sum(x['input_longet_64'])/sample_size}, \\n Ouput mean: {sum(x['output_len'])/sample_size},\\n% of output len > 256: {sum(x['output_longet_256'])/sample_size}, \\n% of output len > 128: {sum(x['output_longet_128'])/sample_size}, \\n% of output len > 64: {sum(x['output_longet_64'])/sample_size}\")\n",
    "\n",
    "output = data_stats.map(compute_and_print, batched=True, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a96d88e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38756 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_to_features(example_batch, padding = \"max_length\"):\n",
    "    inputs = tokenizer.batch_encode_plus(example_batch[\"inputs\"],is_split_into_words=True, max_length=64, truncation=True)\n",
    "    \n",
    "    targets = tokenizer.batch_encode_plus(example_batch[\"target\"], max_length=256,truncation = True)\n",
    "    if padding == \"max_length\":\n",
    "        targets[\"inputs_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in target] for target in targets[\"input_ids\"]\n",
    "        ]\n",
    "    \n",
    "    inputs[\"labels\"] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "train_data = train_dataset.map(convert_to_features, batched=True, remove_columns=train_dataset.column_names)\n",
    "test_data = eval_dataset.map(convert_to_features, batched=True, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35546b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 4307\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd753dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['input_ids', 'attention_mask', 'labels']\n",
    "\n",
    "train_data.set_format(type='torch', columns=columns)\n",
    "test_data.set_format(type='torch', columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b9cfd32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38756"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "375cbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "import os\n",
    "\n",
    "output_dir = 'T5-fine-tuned-with-IMDB-wikisql'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b9a87fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    per_device_eval_batch_size=16,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    do_train=True,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.1,\n",
    "    do_eval=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    overwrite_output_dir=True,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    fp16=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d29d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q rouge_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84447030",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "rouge = load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "570e9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    predictions, labels = pred\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str)\n",
    "    return {\n",
    "        \"rouge1\": round(rouge_output['rouge1']* 100,4),\n",
    "        \"rouge2\": round(rouge_output['rouge2']*100, 4),\n",
    "        \"rougeL\": round(rouge_output['rougeL']*100, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "194641d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c836f916",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset= test_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64966203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toibazd/miniconda3/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='24230' max='24230' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [24230/24230 2:07:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.016623</td>\n",
       "      <td>39.031700</td>\n",
       "      <td>36.486400</td>\n",
       "      <td>39.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.011903</td>\n",
       "      <td>39.032400</td>\n",
       "      <td>36.489200</td>\n",
       "      <td>39.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.009470</td>\n",
       "      <td>39.033700</td>\n",
       "      <td>36.489900</td>\n",
       "      <td>39.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.012700</td>\n",
       "      <td>0.008118</td>\n",
       "      <td>39.033700</td>\n",
       "      <td>36.489900</td>\n",
       "      <td>39.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.007531</td>\n",
       "      <td>39.033000</td>\n",
       "      <td>36.489900</td>\n",
       "      <td>39.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>39.032400</td>\n",
       "      <td>36.489200</td>\n",
       "      <td>39.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.006670</td>\n",
       "      <td>39.033000</td>\n",
       "      <td>36.489900</td>\n",
       "      <td>39.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>39.033800</td>\n",
       "      <td>36.490200</td>\n",
       "      <td>39.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>39.033800</td>\n",
       "      <td>36.490200</td>\n",
       "      <td>39.017400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.006458</td>\n",
       "      <td>39.032400</td>\n",
       "      <td>36.489200</td>\n",
       "      <td>39.015100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=24230, training_loss=0.02062915694944828, metrics={'train_runtime': 7669.4759, 'train_samples_per_second': 50.533, 'train_steps_per_second': 3.159, 'total_flos': 2671447320428544.0, 'train_loss': 0.02062915694944828, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1e7ff2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "831e1de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('T5-fine-tuned-with-IMDB-wikisql/tokenizer_config.json',\n",
       " 'T5-fine-tuned-with-IMDB-wikisql/special_tokens_map.json',\n",
       " 'T5-fine-tuned-with-IMDB-wikisql/tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "283bcf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c0fcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer)\n",
    "text = \"Translate to SQL: how many films by female actors\"\n",
    "\n",
    "input = tokenizer(text, truncation=True, padding = \"max_length\", max_length=64, return_tensors=\"pt\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24e48440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> SELECT d.id AS director_id, d.first_name, d.last_name, COUNT(m.id) AS movie_count FROM actors a JOIN roles r ON a.id = r.actor_id JOIN movies m ON r.movie_id = m.id where d.first_name = 'Federal' and d.last_name = 'Actor_id INNER JOIN movies m ON m.id, d.first_name, d.last_name</s>\n",
      "<pad> SELECT d.id AS director_id, d.first_name, d.last_name, COUNT(m.id) AS movie_count FROM actors a JOIN roles r ON a.id = r.actor_id JOIN movies m ON r.movie_id = m.id where d.first_name = 'Federal' and d.last_name = 'Actor_id INNER JOIN movies m ON m.id, d.first_name, d.last_name</s>\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(**input, max_new_tokens = 256, streamer=streamer)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb94fda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

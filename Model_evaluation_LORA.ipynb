{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9538a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-17 21:26:26,680] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 21:26:33.166782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 21:26:33.350751: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-17 21:26:33.563700: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-17 21:26:37.013873: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-17 21:26:37.014439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-17 21:26:37.014454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): T5ForConditionalGeneration(\n",
       "      (shared): Embedding(32128, 512)\n",
       "      (encoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 512)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 8)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-5): 5 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (decoder): T5Stack(\n",
       "        (embed_tokens): Embedding(32128, 512)\n",
       "        (block): ModuleList(\n",
       "          (0): T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (relative_attention_bias): Embedding(32, 8)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1-5): 5 x T5Block(\n",
       "            (layer): ModuleList(\n",
       "              (0): T5LayerSelfAttention(\n",
       "                (SelfAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): T5LayerCrossAttention(\n",
       "                (EncDecAttention): T5Attention(\n",
       "                  (q): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (v): Linear(\n",
       "                    in_features=512, out_features=512, bias=False\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): T5LayerFF(\n",
       "                (DenseReluDense): T5DenseActDense(\n",
       "                  (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "                  (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (act): ReLU()\n",
       "                )\n",
       "                (layer_norm): T5LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): T5LayerNorm()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "hf_peft_repo = \"LoRA_adapted_T5\"\n",
    "peft_config = PeftConfig.from_pretrained(hf_peft_repo)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(peft_config.base_model_name_or_path, return_dict=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path)\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# Load the finetuned Lora PEFT model\n",
    "model = PeftModel.from_pretrained(model, hf_peft_repo)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75984ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Training_set_IMDB/testing_set_no_target.csv')\n",
    "df = df.sample(frac =1).reset_index(drop=True)\n",
    "for index,row in df.iterrows():\n",
    "    df.loc[index, 'Text'] = \"Translate to SQL: \" + row['Text']\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('Training_set_IMDB/testing_set_unseen_no_target.csv')\n",
    "df2 = df2.sample(frac =1).reset_index(drop=True)\n",
    "for index,row in df2.iterrows():\n",
    "    df2.loc[index, 'Text'] = \"Translate to SQL: \" + row['Text']\n",
    "\n",
    "test_set_seen = Dataset.from_pandas(df)\n",
    "test_set_unseen = Dataset.from_pandas(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b1699b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate to SQL: How many roles by movie 'Skinny Puppy: Video Collection 1984-1992'\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_seen.set_format(type = \"torch\")\n",
    "test_set_unseen.set_format(type = \"torch\")\n",
    "\n",
    "test_set_seen[\"Text\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a4b6240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def map_to_lenght(x):\n",
    "    x[\"input_len\"] = len(tokenizer(x[\"Text\"]).input_ids)\n",
    "    x[\"input_longer_256\"] = int(x[\"input_len\"]>256)\n",
    "    x[\"input_longer_128\"] = int(x[\"input_len\"]>128)\n",
    "    x[\"input_longet_64\"] = int(x[\"input_len\"]>64)\n",
    "    x[\"output_len\"] = len(tokenizer(x[\"SQL\"]).input_ids)\n",
    "    x[\"output_longet_256\"] = int(x[\"output_len\"]>256)\n",
    "    x[\"output_longet_128\"] = int(x[\"output_len\"]>128)\n",
    "    x[\"output_longet_64\"] = int(x[\"output_len\"]>64)\n",
    "    return x\n",
    "\n",
    "sample_size = 2000\n",
    "data_stats = test_set_seen.select(range(sample_size)).map(map_to_lenght, num_proc=4)\n",
    "data_stats_2 = test_set_unseen.select(range(sample_size)).map(map_to_lenght, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b99aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input mean: 19.3125 \n",
      " % of input len > 256: 0.0, \n",
      " % of input len > 128: 0.0, \n",
      " % of input len > 64: 0.0, \n",
      " Ouput mean: 106.75450134277344,\n",
      "% of output len > 256: 0.0, \n",
      "% of output len > 128: 0.30000001192092896, \n",
      "% of output len > 64: 0.7365000247955322\n"
     ]
    }
   ],
   "source": [
    "def compute_and_print(x):\n",
    "    if len(x[\"input_len\"])==sample_size:\n",
    "        print(\n",
    "            f\"Input mean: {sum(x['input_len'])/sample_size} \\n % of input len > 256: {sum(x['input_longer_256'])/sample_size}, \\n % of input len > 128: {sum(x['input_longer_128'])/sample_size}, \\n % of input len > 64: {sum(x['input_longet_64'])/sample_size}, \\n Ouput mean: {sum(x['output_len'])/sample_size},\\n% of output len > 256: {sum(x['output_longet_256'])/sample_size}, \\n% of output len > 128: {sum(x['output_longet_128'])/sample_size}, \\n% of output len > 64: {sum(x['output_longet_64'])/sample_size}\")\n",
    "\n",
    "output = data_stats.map(compute_and_print, batched=True, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6159a20e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Translate to SQL: List all movies by director Robert de Nesle order by rank'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_seen['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1cbcbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example_batch, padding = \"max_length\",input_max = 100, output_max = 170):\n",
    "    inputs = tokenizer.batch_encode_plus(example_batch[\"Text\"], max_length=input_max, is_split_into_words = False, padding='max_length', truncation=True, return_tensors = \"pt\")\n",
    "    \n",
    "    targets = tokenizer.batch_encode_plus(example_batch[\"SQL\"], max_length=output_max, padding = \"max_length\",truncation = True)\n",
    "    if padding == \"max_length\":\n",
    "        targets[\"inputs_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in target] for target in targets[\"input_ids\"]\n",
    "        ]\n",
    "    \n",
    "    inputs[\"labels\"] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "def evaluate_peft_model(sample):\n",
    "    outputs = model.generate(input_ids=sample[\"input_ids\"].unsqueeze(0).cuda(), max_new_tokens = 200, top_p=0.9)\n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "    label = np.where(sample['labels'] != -100, sample['labels'], tokenizer.pad_token_id)\n",
    "    label = tokenizer.decode(label, skip_special_tokens=True)\n",
    "    _ = execution_accuracy(prediction, label)\n",
    "    \n",
    "    return prediction, label\n",
    "\n",
    "def execution_accuracy(prediction, label):\n",
    "    try:\n",
    "        \n",
    "        cursor.execute(label)\n",
    "        result_label = cursor.fetchall()\n",
    "        all_executions_overall.append(1)\n",
    "        try:\n",
    "            cursor.execute(prediction)\n",
    "            result_pred = cursor.fetchall()\n",
    "            all_executions_accuracy.append(1)\n",
    "            if len(result_label)>10:\n",
    "                if len(result_label) == len(result_pred):\n",
    "                    accurate_executions.append(1) \n",
    "            elif result_label == result_pred:\n",
    "                accurate_executions.append(1)\n",
    "            else:\n",
    "                for_checking_label.append(label)\n",
    "                for_checking_prediction.append(prediction)\n",
    "                \n",
    "        except:\n",
    "            failed_executions.append(1)\n",
    "            failed_predicted_SQL.append(prediction)\n",
    "                \n",
    "    except:\n",
    "        failed_original_SQL.append(label)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "080bb7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapping both datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2655 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapped both dataset\n",
      "Documents we have: tokenizer_dataset for seen and tokenized_dataset_2 for unseen data\n",
      "\n",
      "\n",
      " Running executions for seen dataset\n",
      "all_executions_overall:  1\n",
      "all_executions_accuracy:  1\n",
      "accurate_executions:  0\n",
      "failed_executions:  0\n",
      "all_executions_overall:  2\n",
      "all_executions_accuracy:  2\n",
      "accurate_executions:  1\n",
      "failed_executions:  0\n",
      "all_executions_overall:  3\n",
      "all_executions_accuracy:  3\n",
      "accurate_executions:  2\n",
      "failed_executions:  0\n",
      "all_executions_overall:  4\n",
      "all_executions_accuracy:  4\n",
      "accurate_executions:  3\n",
      "failed_executions:  0\n",
      "all_executions_overall:  5\n",
      "all_executions_accuracy:  5\n",
      "accurate_executions:  4\n",
      "failed_executions:  0\n",
      "all_executions_overall:  6\n",
      "all_executions_accuracy:  6\n",
      "accurate_executions:  5\n",
      "failed_executions:  0\n",
      "all_executions_overall:  7\n",
      "all_executions_accuracy:  7\n",
      "accurate_executions:  6\n",
      "failed_executions:  0\n",
      "all_executions_overall:  8\n",
      "all_executions_accuracy:  8\n",
      "accurate_executions:  6\n",
      "failed_executions:  0\n",
      "all_executions_overall:  9\n",
      "all_executions_accuracy:  9\n",
      "accurate_executions:  7\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  9\n",
      "all_executions_accuracy:  9\n",
      "accurate_executions:  7\n",
      "failed_executions:  0\n",
      "all_executions_overall:  10\n",
      "all_executions_accuracy:  10\n",
      "accurate_executions:  8\n",
      "failed_executions:  0\n",
      "all_executions_overall:  11\n",
      "all_executions_accuracy:  11\n",
      "accurate_executions:  9\n",
      "failed_executions:  0\n",
      "all_executions_overall:  12\n",
      "all_executions_accuracy:  12\n",
      "accurate_executions:  10\n",
      "failed_executions:  0\n",
      "all_executions_overall:  13\n",
      "all_executions_accuracy:  13\n",
      "accurate_executions:  11\n",
      "failed_executions:  0\n",
      "all_executions_overall:  14\n",
      "all_executions_accuracy:  14\n",
      "accurate_executions:  12\n",
      "failed_executions:  0\n",
      "all_executions_overall:  15\n",
      "all_executions_accuracy:  15\n",
      "accurate_executions:  13\n",
      "failed_executions:  0\n",
      "all_executions_overall:  16\n",
      "all_executions_accuracy:  16\n",
      "accurate_executions:  14\n",
      "failed_executions:  0\n",
      "all_executions_overall:  17\n",
      "all_executions_accuracy:  17\n",
      "accurate_executions:  14\n",
      "failed_executions:  0\n",
      "all_executions_overall:  18\n",
      "all_executions_accuracy:  18\n",
      "accurate_executions:  15\n",
      "failed_executions:  0\n",
      "all_executions_overall:  19\n",
      "all_executions_accuracy:  19\n",
      "accurate_executions:  16\n",
      "failed_executions:  0\n",
      "all_executions_overall:  20\n",
      "all_executions_accuracy:  20\n",
      "accurate_executions:  17\n",
      "failed_executions:  0\n",
      "all_executions_overall:  21\n",
      "all_executions_accuracy:  21\n",
      "accurate_executions:  17\n",
      "failed_executions:  0\n",
      "all_executions_overall:  22\n",
      "all_executions_accuracy:  22\n",
      "accurate_executions:  18\n",
      "failed_executions:  0\n",
      "all_executions_overall:  23\n",
      "all_executions_accuracy:  23\n",
      "accurate_executions:  19\n",
      "failed_executions:  0\n",
      "all_executions_overall:  24\n",
      "all_executions_accuracy:  24\n",
      "accurate_executions:  20\n",
      "failed_executions:  0\n",
      "all_executions_overall:  25\n",
      "all_executions_accuracy:  25\n",
      "accurate_executions:  21\n",
      "failed_executions:  0\n",
      "all_executions_overall:  26\n",
      "all_executions_accuracy:  26\n",
      "accurate_executions:  22\n",
      "failed_executions:  0\n",
      "all_executions_overall:  27\n",
      "all_executions_accuracy:  27\n",
      "accurate_executions:  23\n",
      "failed_executions:  0\n",
      "all_executions_overall:  28\n",
      "all_executions_accuracy:  28\n",
      "accurate_executions:  24\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  28\n",
      "all_executions_accuracy:  28\n",
      "accurate_executions:  24\n",
      "failed_executions:  0\n",
      "all_executions_overall:  29\n",
      "all_executions_accuracy:  29\n",
      "accurate_executions:  24\n",
      "failed_executions:  0\n",
      "all_executions_overall:  30\n",
      "all_executions_accuracy:  30\n",
      "accurate_executions:  25\n",
      "failed_executions:  0\n",
      "all_executions_overall:  31\n",
      "all_executions_accuracy:  31\n",
      "accurate_executions:  26\n",
      "failed_executions:  0\n",
      "all_executions_overall:  32\n",
      "all_executions_accuracy:  32\n",
      "accurate_executions:  27\n",
      "failed_executions:  0\n",
      "all_executions_overall:  33\n",
      "all_executions_accuracy:  33\n",
      "accurate_executions:  28\n",
      "failed_executions:  0\n",
      "all_executions_overall:  34\n",
      "all_executions_accuracy:  34\n",
      "accurate_executions:  29\n",
      "failed_executions:  0\n",
      "all_executions_overall:  35\n",
      "all_executions_accuracy:  35\n",
      "accurate_executions:  30\n",
      "failed_executions:  0\n",
      "all_executions_overall:  36\n",
      "all_executions_accuracy:  36\n",
      "accurate_executions:  31\n",
      "failed_executions:  0\n",
      "all_executions_overall:  37\n",
      "all_executions_accuracy:  37\n",
      "accurate_executions:  32\n",
      "failed_executions:  0\n",
      "all_executions_overall:  38\n",
      "all_executions_accuracy:  38\n",
      "accurate_executions:  33\n",
      "failed_executions:  0\n",
      "all_executions_overall:  39\n",
      "all_executions_accuracy:  39\n",
      "accurate_executions:  34\n",
      "failed_executions:  0\n",
      "all_executions_overall:  40\n",
      "all_executions_accuracy:  40\n",
      "accurate_executions:  34\n",
      "failed_executions:  0\n",
      "all_executions_overall:  41\n",
      "all_executions_accuracy:  41\n",
      "accurate_executions:  35\n",
      "failed_executions:  0\n",
      "all_executions_overall:  42\n",
      "all_executions_accuracy:  42\n",
      "accurate_executions:  36\n",
      "failed_executions:  0\n",
      "all_executions_overall:  43\n",
      "all_executions_accuracy:  43\n",
      "accurate_executions:  36\n",
      "failed_executions:  0\n",
      "all_executions_overall:  44\n",
      "all_executions_accuracy:  44\n",
      "accurate_executions:  37\n",
      "failed_executions:  0\n",
      "all_executions_overall:  45\n",
      "all_executions_accuracy:  45\n",
      "accurate_executions:  38\n",
      "failed_executions:  0\n",
      "all_executions_overall:  46\n",
      "all_executions_accuracy:  46\n",
      "accurate_executions:  39\n",
      "failed_executions:  0\n",
      "all_executions_overall:  47\n",
      "all_executions_accuracy:  47\n",
      "accurate_executions:  39\n",
      "failed_executions:  0\n",
      "all_executions_overall:  48\n",
      "all_executions_accuracy:  48\n",
      "accurate_executions:  40\n",
      "failed_executions:  0\n",
      "all_executions_overall:  49\n",
      "all_executions_accuracy:  49\n",
      "accurate_executions:  41\n",
      "failed_executions:  0\n",
      "all_executions_overall:  50\n",
      "all_executions_accuracy:  50\n",
      "accurate_executions:  42\n",
      "failed_executions:  0\n",
      "all_executions_overall:  51\n",
      "all_executions_accuracy:  51\n",
      "accurate_executions:  43\n",
      "failed_executions:  0\n",
      "all_executions_overall:  52\n",
      "all_executions_accuracy:  52\n",
      "accurate_executions:  43\n",
      "failed_executions:  0\n",
      "all_executions_overall:  53\n",
      "all_executions_accuracy:  53\n",
      "accurate_executions:  44\n",
      "failed_executions:  0\n",
      "all_executions_overall:  54\n",
      "all_executions_accuracy:  54\n",
      "accurate_executions:  44\n",
      "failed_executions:  0\n",
      "all_executions_overall:  55\n",
      "all_executions_accuracy:  55\n",
      "accurate_executions:  45\n",
      "failed_executions:  0\n",
      "all_executions_overall:  56\n",
      "all_executions_accuracy:  56\n",
      "accurate_executions:  46\n",
      "failed_executions:  0\n",
      "all_executions_overall:  57\n",
      "all_executions_accuracy:  57\n",
      "accurate_executions:  46\n",
      "failed_executions:  0\n",
      "all_executions_overall:  58\n",
      "all_executions_accuracy:  58\n",
      "accurate_executions:  47\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  58\n",
      "all_executions_accuracy:  58\n",
      "accurate_executions:  47\n",
      "failed_executions:  0\n",
      "all_executions_overall:  59\n",
      "all_executions_accuracy:  59\n",
      "accurate_executions:  48\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  59\n",
      "all_executions_accuracy:  59\n",
      "accurate_executions:  48\n",
      "failed_executions:  0\n",
      "all_executions_overall:  60\n",
      "all_executions_accuracy:  60\n",
      "accurate_executions:  49\n",
      "failed_executions:  0\n",
      "all_executions_overall:  61\n",
      "all_executions_accuracy:  61\n",
      "accurate_executions:  49\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  61\n",
      "all_executions_accuracy:  61\n",
      "accurate_executions:  49\n",
      "failed_executions:  0\n",
      "all_executions_overall:  62\n",
      "all_executions_accuracy:  62\n",
      "accurate_executions:  50\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  62\n",
      "all_executions_accuracy:  62\n",
      "accurate_executions:  50\n",
      "failed_executions:  0\n",
      "all_executions_overall:  63\n",
      "all_executions_accuracy:  63\n",
      "accurate_executions:  51\n",
      "failed_executions:  0\n",
      "all_executions_overall:  64\n",
      "all_executions_accuracy:  64\n",
      "accurate_executions:  52\n",
      "failed_executions:  0\n",
      "all_executions_overall:  65\n",
      "all_executions_accuracy:  65\n",
      "accurate_executions:  53\n",
      "failed_executions:  0\n",
      "all_executions_overall:  66\n",
      "all_executions_accuracy:  66\n",
      "accurate_executions:  54\n",
      "failed_executions:  0\n",
      "all_executions_overall:  67\n",
      "all_executions_accuracy:  67\n",
      "accurate_executions:  55\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  67\n",
      "all_executions_accuracy:  67\n",
      "accurate_executions:  55\n",
      "failed_executions:  0\n",
      "all_executions_overall:  68\n",
      "all_executions_accuracy:  68\n",
      "accurate_executions:  55\n",
      "failed_executions:  0\n",
      "all_executions_overall:  69\n",
      "all_executions_accuracy:  69\n",
      "accurate_executions:  55\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  69\n",
      "all_executions_accuracy:  69\n",
      "accurate_executions:  55\n",
      "failed_executions:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_executions_overall:  70\n",
      "all_executions_accuracy:  70\n",
      "accurate_executions:  56\n",
      "failed_executions:  0\n",
      "all_executions_overall:  71\n",
      "all_executions_accuracy:  71\n",
      "accurate_executions:  57\n",
      "failed_executions:  0\n",
      "all_executions_overall:  72\n",
      "all_executions_accuracy:  72\n",
      "accurate_executions:  58\n",
      "failed_executions:  0\n",
      "all_executions_overall:  73\n",
      "all_executions_accuracy:  73\n",
      "accurate_executions:  59\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  73\n",
      "all_executions_accuracy:  73\n",
      "accurate_executions:  59\n",
      "failed_executions:  0\n",
      "all_executions_overall:  74\n",
      "all_executions_accuracy:  74\n",
      "accurate_executions:  59\n",
      "failed_executions:  0\n",
      "all_executions_overall:  75\n",
      "all_executions_accuracy:  75\n",
      "accurate_executions:  60\n",
      "failed_executions:  0\n",
      "all_executions_overall:  76\n",
      "all_executions_accuracy:  76\n",
      "accurate_executions:  60\n",
      "failed_executions:  0\n",
      "all_executions_overall:  77\n",
      "all_executions_accuracy:  77\n",
      "accurate_executions:  61\n",
      "failed_executions:  0\n",
      "all_executions_overall:  78\n",
      "all_executions_accuracy:  78\n",
      "accurate_executions:  62\n",
      "failed_executions:  0\n",
      "all_executions_overall:  79\n",
      "all_executions_accuracy:  79\n",
      "accurate_executions:  62\n",
      "failed_executions:  0\n",
      "all_executions_overall:  80\n",
      "all_executions_accuracy:  80\n",
      "accurate_executions:  63\n",
      "failed_executions:  0\n",
      "all_executions_overall:  81\n",
      "all_executions_accuracy:  81\n",
      "accurate_executions:  63\n",
      "failed_executions:  0\n",
      "all_executions_overall:  82\n",
      "all_executions_accuracy:  82\n",
      "accurate_executions:  64\n",
      "failed_executions:  0\n",
      "all_executions_overall:  83\n",
      "all_executions_accuracy:  83\n",
      "accurate_executions:  65\n",
      "failed_executions:  0\n",
      "all_executions_overall:  84\n",
      "all_executions_accuracy:  84\n",
      "accurate_executions:  65\n",
      "failed_executions:  0\n",
      "all_executions_overall:  85\n",
      "all_executions_accuracy:  85\n",
      "accurate_executions:  66\n",
      "failed_executions:  0\n",
      "all_executions_overall:  86\n",
      "all_executions_accuracy:  86\n",
      "accurate_executions:  67\n",
      "failed_executions:  0\n",
      "all_executions_overall:  87\n",
      "all_executions_accuracy:  87\n",
      "accurate_executions:  68\n",
      "failed_executions:  0\n",
      "all_executions_overall:  88\n",
      "all_executions_accuracy:  88\n",
      "accurate_executions:  69\n",
      "failed_executions:  0\n",
      "all_executions_overall:  89\n",
      "all_executions_accuracy:  89\n",
      "accurate_executions:  70\n",
      "failed_executions:  0\n",
      "all_executions_overall:  90\n",
      "all_executions_accuracy:  90\n",
      "accurate_executions:  71\n",
      "failed_executions:  0\n",
      "all_executions_overall:  91\n",
      "all_executions_accuracy:  91\n",
      "accurate_executions:  72\n",
      "failed_executions:  0\n",
      "all_executions_overall:  92\n",
      "all_executions_accuracy:  92\n",
      "accurate_executions:  73\n",
      "failed_executions:  0\n",
      "all_executions_overall:  93\n",
      "all_executions_accuracy:  93\n",
      "accurate_executions:  74\n",
      "failed_executions:  0\n",
      "all_executions_overall:  94\n",
      "all_executions_accuracy:  94\n",
      "accurate_executions:  75\n",
      "failed_executions:  0\n",
      "all_executions_overall:  95\n",
      "all_executions_accuracy:  95\n",
      "accurate_executions:  75\n",
      "failed_executions:  0\n",
      "all_executions_overall:  96\n",
      "all_executions_accuracy:  96\n",
      "accurate_executions:  76\n",
      "failed_executions:  0\n",
      "all_executions_overall:  97\n",
      "all_executions_accuracy:  97\n",
      "accurate_executions:  77\n",
      "failed_executions:  0\n",
      "all_executions_overall:  98\n",
      "all_executions_accuracy:  98\n",
      "accurate_executions:  78\n",
      "failed_executions:  0\n",
      "all_executions_overall:  99\n",
      "all_executions_accuracy:  99\n",
      "accurate_executions:  79\n",
      "failed_executions:  0\n",
      "all_executions_overall:  100\n",
      "all_executions_accuracy:  100\n",
      "accurate_executions:  80\n",
      "failed_executions:  0\n",
      "all_executions_overall:  101\n",
      "all_executions_accuracy:  101\n",
      "accurate_executions:  81\n",
      "failed_executions:  0\n",
      "all_executions_overall:  102\n",
      "all_executions_accuracy:  102\n",
      "accurate_executions:  82\n",
      "failed_executions:  0\n",
      "all_executions_overall:  103\n",
      "all_executions_accuracy:  103\n",
      "accurate_executions:  83\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  103\n",
      "all_executions_accuracy:  103\n",
      "accurate_executions:  83\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  103\n",
      "all_executions_accuracy:  103\n",
      "accurate_executions:  83\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  103\n",
      "all_executions_accuracy:  103\n",
      "accurate_executions:  83\n",
      "failed_executions:  0\n",
      "all_executions_overall:  104\n",
      "all_executions_accuracy:  104\n",
      "accurate_executions:  84\n",
      "failed_executions:  0\n",
      "all_executions_overall:  105\n",
      "all_executions_accuracy:  105\n",
      "accurate_executions:  84\n",
      "failed_executions:  0\n",
      "all_executions_overall:  106\n",
      "all_executions_accuracy:  106\n",
      "accurate_executions:  85\n",
      "failed_executions:  0\n",
      "all_executions_overall:  107\n",
      "all_executions_accuracy:  107\n",
      "accurate_executions:  86\n",
      "failed_executions:  0\n",
      "all_executions_overall:  108\n",
      "all_executions_accuracy:  108\n",
      "accurate_executions:  87\n",
      "failed_executions:  0\n",
      "all_executions_overall:  109\n",
      "all_executions_accuracy:  109\n",
      "accurate_executions:  88\n",
      "failed_executions:  0\n",
      "all_executions_overall:  110\n",
      "all_executions_accuracy:  110\n",
      "accurate_executions:  89\n",
      "failed_executions:  0\n",
      "all_executions_overall:  111\n",
      "all_executions_accuracy:  111\n",
      "accurate_executions:  90\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  111\n",
      "all_executions_accuracy:  111\n",
      "accurate_executions:  90\n",
      "failed_executions:  0\n",
      "all_executions_overall:  112\n",
      "all_executions_accuracy:  112\n",
      "accurate_executions:  90\n",
      "failed_executions:  0\n",
      "all_executions_overall:  113\n",
      "all_executions_accuracy:  113\n",
      "accurate_executions:  90\n",
      "failed_executions:  0\n",
      "all_executions_overall:  114\n",
      "all_executions_accuracy:  114\n",
      "accurate_executions:  91\n",
      "failed_executions:  0\n",
      "all_executions_overall:  115\n",
      "all_executions_accuracy:  115\n",
      "accurate_executions:  92\n",
      "failed_executions:  0\n",
      "all_executions_overall:  116\n",
      "all_executions_accuracy:  116\n",
      "accurate_executions:  93\n",
      "failed_executions:  0\n",
      "all_executions_overall:  117\n",
      "all_executions_accuracy:  117\n",
      "accurate_executions:  94\n",
      "failed_executions:  0\n",
      "all_executions_overall:  118\n",
      "all_executions_accuracy:  118\n",
      "accurate_executions:  95\n",
      "failed_executions:  0\n",
      "all_executions_overall:  119\n",
      "all_executions_accuracy:  119\n",
      "accurate_executions:  96\n",
      "failed_executions:  0\n",
      "all_executions_overall:  120\n",
      "all_executions_accuracy:  120\n",
      "accurate_executions:  97\n",
      "failed_executions:  0\n",
      "all_executions_overall:  121\n",
      "all_executions_accuracy:  121\n",
      "accurate_executions:  98\n",
      "failed_executions:  0\n",
      "all_executions_overall:  122\n",
      "all_executions_accuracy:  122\n",
      "accurate_executions:  99\n",
      "failed_executions:  0\n",
      "all_executions_overall:  123\n",
      "all_executions_accuracy:  123\n",
      "accurate_executions:  100\n",
      "failed_executions:  0\n",
      "all_executions_overall:  124\n",
      "all_executions_accuracy:  124\n",
      "accurate_executions:  100\n",
      "failed_executions:  0\n",
      "all_executions_overall:  125\n",
      "all_executions_accuracy:  125\n",
      "accurate_executions:  100\n",
      "failed_executions:  0\n",
      "all_executions_overall:  126\n",
      "all_executions_accuracy:  126\n",
      "accurate_executions:  101\n",
      "failed_executions:  0\n",
      "all_executions_overall:  127\n",
      "all_executions_accuracy:  127\n",
      "accurate_executions:  102\n",
      "failed_executions:  0\n",
      "all_executions_overall:  128\n",
      "all_executions_accuracy:  128\n",
      "accurate_executions:  103\n",
      "failed_executions:  0\n",
      "all_executions_overall:  129\n",
      "all_executions_accuracy:  129\n",
      "accurate_executions:  104\n",
      "failed_executions:  0\n",
      "all_executions_overall:  130\n",
      "all_executions_accuracy:  130\n",
      "accurate_executions:  105\n",
      "failed_executions:  0\n",
      "all_executions_overall:  131\n",
      "all_executions_accuracy:  131\n",
      "accurate_executions:  105\n",
      "failed_executions:  0\n",
      "all_executions_overall:  132\n",
      "all_executions_accuracy:  132\n",
      "accurate_executions:  106\n",
      "failed_executions:  0\n",
      "all_executions_overall:  133\n",
      "all_executions_accuracy:  133\n",
      "accurate_executions:  107\n",
      "failed_executions:  0\n",
      "all_executions_overall:  134\n",
      "all_executions_accuracy:  134\n",
      "accurate_executions:  108\n",
      "failed_executions:  0\n",
      "all_executions_overall:  135\n",
      "all_executions_accuracy:  135\n",
      "accurate_executions:  108\n",
      "failed_executions:  0\n",
      "all_executions_overall:  136\n",
      "all_executions_accuracy:  136\n",
      "accurate_executions:  109\n",
      "failed_executions:  0\n",
      "all_executions_overall:  137\n",
      "all_executions_accuracy:  137\n",
      "accurate_executions:  110\n",
      "failed_executions:  0\n",
      "all_executions_overall:  138\n",
      "all_executions_accuracy:  138\n",
      "accurate_executions:  111\n",
      "failed_executions:  0\n",
      "all_executions_overall:  139\n",
      "all_executions_accuracy:  139\n",
      "accurate_executions:  111\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  139\n",
      "all_executions_accuracy:  139\n",
      "accurate_executions:  111\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  139\n",
      "all_executions_accuracy:  139\n",
      "accurate_executions:  111\n",
      "failed_executions:  0\n",
      "all_executions_overall:  140\n",
      "all_executions_accuracy:  140\n",
      "accurate_executions:  112\n",
      "failed_executions:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except_1\n",
      "all_executions_overall:  140\n",
      "all_executions_accuracy:  140\n",
      "accurate_executions:  112\n",
      "failed_executions:  0\n",
      "all_executions_overall:  141\n",
      "all_executions_accuracy:  141\n",
      "accurate_executions:  113\n",
      "failed_executions:  0\n",
      "all_executions_overall:  142\n",
      "all_executions_accuracy:  142\n",
      "accurate_executions:  114\n",
      "failed_executions:  0\n",
      "all_executions_overall:  143\n",
      "all_executions_accuracy:  143\n",
      "accurate_executions:  115\n",
      "failed_executions:  0\n",
      "all_executions_overall:  144\n",
      "all_executions_accuracy:  144\n",
      "accurate_executions:  116\n",
      "failed_executions:  0\n",
      "all_executions_overall:  145\n",
      "all_executions_accuracy:  145\n",
      "accurate_executions:  117\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  145\n",
      "all_executions_accuracy:  145\n",
      "accurate_executions:  117\n",
      "failed_executions:  0\n",
      "all_executions_overall:  146\n",
      "all_executions_accuracy:  146\n",
      "accurate_executions:  118\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  146\n",
      "all_executions_accuracy:  146\n",
      "accurate_executions:  118\n",
      "failed_executions:  0\n",
      "all_executions_overall:  147\n",
      "all_executions_accuracy:  147\n",
      "accurate_executions:  119\n",
      "failed_executions:  0\n",
      "all_executions_overall:  148\n",
      "all_executions_accuracy:  148\n",
      "accurate_executions:  119\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  148\n",
      "all_executions_accuracy:  148\n",
      "accurate_executions:  119\n",
      "failed_executions:  0\n",
      "all_executions_overall:  149\n",
      "all_executions_accuracy:  149\n",
      "accurate_executions:  120\n",
      "failed_executions:  0\n",
      "all_executions_overall:  150\n",
      "all_executions_accuracy:  150\n",
      "accurate_executions:  120\n",
      "failed_executions:  0\n",
      "all_executions_overall:  151\n",
      "all_executions_accuracy:  151\n",
      "accurate_executions:  121\n",
      "failed_executions:  0\n",
      "all_executions_overall:  152\n",
      "all_executions_accuracy:  152\n",
      "accurate_executions:  122\n",
      "failed_executions:  0\n",
      "all_executions_overall:  153\n",
      "all_executions_accuracy:  153\n",
      "accurate_executions:  123\n",
      "failed_executions:  0\n",
      "all_executions_overall:  154\n",
      "all_executions_accuracy:  154\n",
      "accurate_executions:  124\n",
      "failed_executions:  0\n",
      "all_executions_overall:  155\n",
      "all_executions_accuracy:  155\n",
      "accurate_executions:  125\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  155\n",
      "all_executions_accuracy:  155\n",
      "accurate_executions:  125\n",
      "failed_executions:  0\n",
      "all_executions_overall:  156\n",
      "all_executions_accuracy:  156\n",
      "accurate_executions:  126\n",
      "failed_executions:  0\n",
      "all_executions_overall:  157\n",
      "all_executions_accuracy:  157\n",
      "accurate_executions:  127\n",
      "failed_executions:  0\n",
      "all_executions_overall:  158\n",
      "all_executions_accuracy:  158\n",
      "accurate_executions:  128\n",
      "failed_executions:  0\n",
      "all_executions_overall:  159\n",
      "all_executions_accuracy:  159\n",
      "accurate_executions:  129\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  159\n",
      "all_executions_accuracy:  159\n",
      "accurate_executions:  129\n",
      "failed_executions:  0\n",
      "all_executions_overall:  160\n",
      "all_executions_accuracy:  160\n",
      "accurate_executions:  130\n",
      "failed_executions:  0\n",
      "all_executions_overall:  161\n",
      "all_executions_accuracy:  161\n",
      "accurate_executions:  131\n",
      "failed_executions:  0\n",
      "all_executions_overall:  162\n",
      "all_executions_accuracy:  162\n",
      "accurate_executions:  131\n",
      "failed_executions:  0\n",
      "all_executions_overall:  163\n",
      "all_executions_accuracy:  163\n",
      "accurate_executions:  131\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  163\n",
      "all_executions_accuracy:  163\n",
      "accurate_executions:  131\n",
      "failed_executions:  0\n",
      "all_executions_overall:  164\n",
      "all_executions_accuracy:  164\n",
      "accurate_executions:  132\n",
      "failed_executions:  0\n",
      "all_executions_overall:  165\n",
      "all_executions_accuracy:  165\n",
      "accurate_executions:  133\n",
      "failed_executions:  0\n",
      "all_executions_overall:  166\n",
      "all_executions_accuracy:  166\n",
      "accurate_executions:  134\n",
      "failed_executions:  0\n",
      "all_executions_overall:  167\n",
      "all_executions_accuracy:  167\n",
      "accurate_executions:  135\n",
      "failed_executions:  0\n",
      "all_executions_overall:  168\n",
      "all_executions_accuracy:  168\n",
      "accurate_executions:  136\n",
      "failed_executions:  0\n",
      "all_executions_overall:  169\n",
      "all_executions_accuracy:  169\n",
      "accurate_executions:  137\n",
      "failed_executions:  0\n",
      "all_executions_overall:  170\n",
      "all_executions_accuracy:  170\n",
      "accurate_executions:  138\n",
      "failed_executions:  0\n",
      "all_executions_overall:  171\n",
      "all_executions_accuracy:  171\n",
      "accurate_executions:  139\n",
      "failed_executions:  0\n",
      "all_executions_overall:  172\n",
      "all_executions_accuracy:  172\n",
      "accurate_executions:  140\n",
      "failed_executions:  0\n",
      "all_executions_overall:  173\n",
      "all_executions_accuracy:  173\n",
      "accurate_executions:  141\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  173\n",
      "all_executions_accuracy:  173\n",
      "accurate_executions:  141\n",
      "failed_executions:  0\n",
      "all_executions_overall:  174\n",
      "all_executions_accuracy:  174\n",
      "accurate_executions:  142\n",
      "failed_executions:  0\n",
      "all_executions_overall:  175\n",
      "all_executions_accuracy:  175\n",
      "accurate_executions:  143\n",
      "failed_executions:  0\n",
      "all_executions_overall:  176\n",
      "all_executions_accuracy:  176\n",
      "accurate_executions:  144\n",
      "failed_executions:  0\n",
      "all_executions_overall:  177\n",
      "all_executions_accuracy:  177\n",
      "accurate_executions:  145\n",
      "failed_executions:  0\n",
      "all_executions_overall:  178\n",
      "all_executions_accuracy:  178\n",
      "accurate_executions:  145\n",
      "failed_executions:  0\n",
      "all_executions_overall:  179\n",
      "all_executions_accuracy:  179\n",
      "accurate_executions:  146\n",
      "failed_executions:  0\n",
      "all_executions_overall:  180\n",
      "all_executions_accuracy:  180\n",
      "accurate_executions:  147\n",
      "failed_executions:  0\n",
      "all_executions_overall:  181\n",
      "all_executions_accuracy:  181\n",
      "accurate_executions:  147\n",
      "failed_executions:  0\n",
      "all_executions_overall:  182\n",
      "all_executions_accuracy:  182\n",
      "accurate_executions:  148\n",
      "failed_executions:  0\n",
      "all_executions_overall:  183\n",
      "all_executions_accuracy:  183\n",
      "accurate_executions:  149\n",
      "failed_executions:  0\n",
      "all_executions_overall:  184\n",
      "all_executions_accuracy:  184\n",
      "accurate_executions:  150\n",
      "failed_executions:  0\n",
      "all_executions_overall:  185\n",
      "all_executions_accuracy:  185\n",
      "accurate_executions:  151\n",
      "failed_executions:  0\n",
      "all_executions_overall:  186\n",
      "all_executions_accuracy:  186\n",
      "accurate_executions:  152\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  186\n",
      "all_executions_accuracy:  186\n",
      "accurate_executions:  152\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  186\n",
      "all_executions_accuracy:  186\n",
      "accurate_executions:  152\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  186\n",
      "all_executions_accuracy:  186\n",
      "accurate_executions:  152\n",
      "failed_executions:  0\n",
      "all_executions_overall:  187\n",
      "all_executions_accuracy:  187\n",
      "accurate_executions:  153\n",
      "failed_executions:  0\n",
      "all_executions_overall:  188\n",
      "all_executions_accuracy:  188\n",
      "accurate_executions:  154\n",
      "failed_executions:  0\n",
      "all_executions_overall:  189\n",
      "all_executions_accuracy:  189\n",
      "accurate_executions:  154\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  189\n",
      "all_executions_accuracy:  189\n",
      "accurate_executions:  154\n",
      "failed_executions:  0\n",
      "all_executions_overall:  190\n",
      "all_executions_accuracy:  190\n",
      "accurate_executions:  155\n",
      "failed_executions:  0\n",
      "all_executions_overall:  191\n",
      "all_executions_accuracy:  191\n",
      "accurate_executions:  156\n",
      "failed_executions:  0\n",
      "all_executions_overall:  192\n",
      "all_executions_accuracy:  192\n",
      "accurate_executions:  156\n",
      "failed_executions:  0\n",
      "all_executions_overall:  193\n",
      "all_executions_accuracy:  193\n",
      "accurate_executions:  157\n",
      "failed_executions:  0\n",
      "all_executions_overall:  194\n",
      "all_executions_accuracy:  194\n",
      "accurate_executions:  157\n",
      "failed_executions:  0\n",
      "all_executions_overall:  195\n",
      "all_executions_accuracy:  195\n",
      "accurate_executions:  158\n",
      "failed_executions:  0\n",
      "all_executions_overall:  196\n",
      "all_executions_accuracy:  196\n",
      "accurate_executions:  159\n",
      "failed_executions:  0\n",
      "all_executions_overall:  197\n",
      "all_executions_accuracy:  197\n",
      "accurate_executions:  160\n",
      "failed_executions:  0\n",
      "all_executions_overall:  198\n",
      "all_executions_accuracy:  198\n",
      "accurate_executions:  161\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  198\n",
      "all_executions_accuracy:  198\n",
      "accurate_executions:  161\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  198\n",
      "all_executions_accuracy:  198\n",
      "accurate_executions:  161\n",
      "failed_executions:  0\n",
      "all_executions_overall:  199\n",
      "all_executions_accuracy:  199\n",
      "accurate_executions:  162\n",
      "failed_executions:  0\n",
      "all_executions_overall:  200\n",
      "all_executions_accuracy:  200\n",
      "accurate_executions:  163\n",
      "failed_executions:  0\n",
      "all_executions_overall:  201\n",
      "all_executions_accuracy:  201\n",
      "accurate_executions:  164\n",
      "failed_executions:  0\n",
      "all_executions_overall:  202\n",
      "all_executions_accuracy:  202\n",
      "accurate_executions:  165\n",
      "failed_executions:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "except_1\n",
      "all_executions_overall:  202\n",
      "all_executions_accuracy:  202\n",
      "accurate_executions:  165\n",
      "failed_executions:  0\n",
      "all_executions_overall:  203\n",
      "all_executions_accuracy:  203\n",
      "accurate_executions:  166\n",
      "failed_executions:  0\n",
      "all_executions_overall:  204\n",
      "all_executions_accuracy:  204\n",
      "accurate_executions:  167\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  204\n",
      "all_executions_accuracy:  204\n",
      "accurate_executions:  167\n",
      "failed_executions:  0\n",
      "all_executions_overall:  205\n",
      "all_executions_accuracy:  205\n",
      "accurate_executions:  167\n",
      "failed_executions:  0\n",
      "all_executions_overall:  206\n",
      "all_executions_accuracy:  206\n",
      "accurate_executions:  168\n",
      "failed_executions:  0\n",
      "all_executions_overall:  207\n",
      "all_executions_accuracy:  207\n",
      "accurate_executions:  169\n",
      "failed_executions:  0\n",
      "all_executions_overall:  208\n",
      "all_executions_accuracy:  208\n",
      "accurate_executions:  170\n",
      "failed_executions:  0\n",
      "all_executions_overall:  209\n",
      "all_executions_accuracy:  209\n",
      "accurate_executions:  171\n",
      "failed_executions:  0\n",
      "all_executions_overall:  210\n",
      "all_executions_accuracy:  210\n",
      "accurate_executions:  172\n",
      "failed_executions:  0\n",
      "all_executions_overall:  211\n",
      "all_executions_accuracy:  211\n",
      "accurate_executions:  173\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  211\n",
      "all_executions_accuracy:  211\n",
      "accurate_executions:  173\n",
      "failed_executions:  0\n",
      "all_executions_overall:  212\n",
      "all_executions_accuracy:  212\n",
      "accurate_executions:  174\n",
      "failed_executions:  0\n",
      "all_executions_overall:  213\n",
      "all_executions_accuracy:  213\n",
      "accurate_executions:  175\n",
      "failed_executions:  0\n",
      "all_executions_overall:  214\n",
      "all_executions_accuracy:  214\n",
      "accurate_executions:  176\n",
      "failed_executions:  0\n",
      "all_executions_overall:  215\n",
      "all_executions_accuracy:  215\n",
      "accurate_executions:  177\n",
      "failed_executions:  0\n",
      "all_executions_overall:  216\n",
      "all_executions_accuracy:  216\n",
      "accurate_executions:  178\n",
      "failed_executions:  0\n",
      "all_executions_overall:  217\n",
      "all_executions_accuracy:  217\n",
      "accurate_executions:  179\n",
      "failed_executions:  0\n",
      "all_executions_overall:  218\n",
      "all_executions_accuracy:  218\n",
      "accurate_executions:  180\n",
      "failed_executions:  0\n",
      "all_executions_overall:  219\n",
      "all_executions_accuracy:  219\n",
      "accurate_executions:  181\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  219\n",
      "all_executions_accuracy:  219\n",
      "accurate_executions:  181\n",
      "failed_executions:  0\n",
      "all_executions_overall:  220\n",
      "all_executions_accuracy:  220\n",
      "accurate_executions:  182\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  220\n",
      "all_executions_accuracy:  220\n",
      "accurate_executions:  182\n",
      "failed_executions:  0\n",
      "all_executions_overall:  221\n",
      "all_executions_accuracy:  221\n",
      "accurate_executions:  183\n",
      "failed_executions:  0\n",
      "all_executions_overall:  222\n",
      "all_executions_accuracy:  222\n",
      "accurate_executions:  184\n",
      "failed_executions:  0\n",
      "all_executions_overall:  223\n",
      "all_executions_accuracy:  223\n",
      "accurate_executions:  185\n",
      "failed_executions:  0\n",
      "all_executions_overall:  224\n",
      "all_executions_accuracy:  224\n",
      "accurate_executions:  186\n",
      "failed_executions:  0\n",
      "all_executions_overall:  225\n",
      "all_executions_accuracy:  225\n",
      "accurate_executions:  187\n",
      "failed_executions:  0\n",
      "all_executions_overall:  226\n",
      "all_executions_accuracy:  226\n",
      "accurate_executions:  187\n",
      "failed_executions:  0\n",
      "all_executions_overall:  227\n",
      "all_executions_accuracy:  227\n",
      "accurate_executions:  188\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  227\n",
      "all_executions_accuracy:  227\n",
      "accurate_executions:  188\n",
      "failed_executions:  0\n",
      "all_executions_overall:  228\n",
      "all_executions_accuracy:  228\n",
      "accurate_executions:  189\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  228\n",
      "all_executions_accuracy:  228\n",
      "accurate_executions:  189\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  228\n",
      "all_executions_accuracy:  228\n",
      "accurate_executions:  189\n",
      "failed_executions:  0\n",
      "all_executions_overall:  229\n",
      "all_executions_accuracy:  229\n",
      "accurate_executions:  190\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  229\n",
      "all_executions_accuracy:  229\n",
      "accurate_executions:  190\n",
      "failed_executions:  0\n",
      "all_executions_overall:  230\n",
      "all_executions_accuracy:  230\n",
      "accurate_executions:  191\n",
      "failed_executions:  0\n",
      "all_executions_overall:  231\n",
      "all_executions_accuracy:  231\n",
      "accurate_executions:  192\n",
      "failed_executions:  0\n",
      "all_executions_overall:  232\n",
      "all_executions_accuracy:  232\n",
      "accurate_executions:  192\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  232\n",
      "all_executions_accuracy:  232\n",
      "accurate_executions:  192\n",
      "failed_executions:  0\n",
      "all_executions_overall:  233\n",
      "all_executions_accuracy:  233\n",
      "accurate_executions:  192\n",
      "failed_executions:  0\n",
      "all_executions_overall:  234\n",
      "all_executions_accuracy:  234\n",
      "accurate_executions:  193\n",
      "failed_executions:  0\n",
      "all_executions_overall:  235\n",
      "all_executions_accuracy:  235\n",
      "accurate_executions:  194\n",
      "failed_executions:  0\n",
      "all_executions_overall:  236\n",
      "all_executions_accuracy:  236\n",
      "accurate_executions:  195\n",
      "failed_executions:  0\n",
      "all_executions_overall:  237\n",
      "all_executions_accuracy:  237\n",
      "accurate_executions:  195\n",
      "failed_executions:  0\n",
      "all_executions_overall:  238\n",
      "all_executions_accuracy:  238\n",
      "accurate_executions:  196\n",
      "failed_executions:  0\n",
      "all_executions_overall:  239\n",
      "all_executions_accuracy:  239\n",
      "accurate_executions:  196\n",
      "failed_executions:  0\n",
      "all_executions_overall:  240\n",
      "all_executions_accuracy:  240\n",
      "accurate_executions:  196\n",
      "failed_executions:  0\n",
      "all_executions_overall:  241\n",
      "all_executions_accuracy:  241\n",
      "accurate_executions:  197\n",
      "failed_executions:  0\n",
      "all_executions_overall:  242\n",
      "all_executions_accuracy:  242\n",
      "accurate_executions:  198\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  242\n",
      "all_executions_accuracy:  242\n",
      "accurate_executions:  198\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  242\n",
      "all_executions_accuracy:  242\n",
      "accurate_executions:  198\n",
      "failed_executions:  0\n",
      "all_executions_overall:  243\n",
      "all_executions_accuracy:  243\n",
      "accurate_executions:  199\n",
      "failed_executions:  0\n",
      "all_executions_overall:  244\n",
      "all_executions_accuracy:  244\n",
      "accurate_executions:  200\n",
      "failed_executions:  0\n",
      "all_executions_overall:  245\n",
      "all_executions_accuracy:  245\n",
      "accurate_executions:  200\n",
      "failed_executions:  0\n",
      "all_executions_overall:  246\n",
      "all_executions_accuracy:  246\n",
      "accurate_executions:  201\n",
      "failed_executions:  0\n",
      "all_executions_overall:  247\n",
      "all_executions_accuracy:  247\n",
      "accurate_executions:  202\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  247\n",
      "all_executions_accuracy:  247\n",
      "accurate_executions:  202\n",
      "failed_executions:  0\n",
      "all_executions_overall:  248\n",
      "all_executions_accuracy:  248\n",
      "accurate_executions:  203\n",
      "failed_executions:  0\n",
      "all_executions_overall:  249\n",
      "all_executions_accuracy:  249\n",
      "accurate_executions:  204\n",
      "failed_executions:  0\n",
      "all_executions_overall:  250\n",
      "all_executions_accuracy:  250\n",
      "accurate_executions:  205\n",
      "failed_executions:  0\n",
      "all_executions_overall:  251\n",
      "all_executions_accuracy:  251\n",
      "accurate_executions:  206\n",
      "failed_executions:  0\n",
      "all_executions_overall:  252\n",
      "all_executions_accuracy:  252\n",
      "accurate_executions:  207\n",
      "failed_executions:  0\n",
      "all_executions_overall:  253\n",
      "all_executions_accuracy:  253\n",
      "accurate_executions:  207\n",
      "failed_executions:  0\n",
      "all_executions_overall:  254\n",
      "all_executions_accuracy:  254\n",
      "accurate_executions:  208\n",
      "failed_executions:  0\n",
      "all_executions_overall:  255\n",
      "all_executions_accuracy:  255\n",
      "accurate_executions:  209\n",
      "failed_executions:  0\n",
      "all_executions_overall:  256\n",
      "all_executions_accuracy:  256\n",
      "accurate_executions:  210\n",
      "failed_executions:  0\n",
      "all_executions_overall:  257\n",
      "all_executions_accuracy:  257\n",
      "accurate_executions:  211\n",
      "failed_executions:  0\n",
      "all_executions_overall:  258\n",
      "all_executions_accuracy:  258\n",
      "accurate_executions:  212\n",
      "failed_executions:  0\n",
      "all_executions_overall:  259\n",
      "all_executions_accuracy:  259\n",
      "accurate_executions:  213\n",
      "failed_executions:  0\n",
      "all_executions_overall:  260\n",
      "all_executions_accuracy:  260\n",
      "accurate_executions:  214\n",
      "failed_executions:  0\n",
      "all_executions_overall:  261\n",
      "all_executions_accuracy:  261\n",
      "accurate_executions:  215\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  261\n",
      "all_executions_accuracy:  261\n",
      "accurate_executions:  215\n",
      "failed_executions:  0\n",
      "all_executions_overall:  262\n",
      "all_executions_accuracy:  262\n",
      "accurate_executions:  216\n",
      "failed_executions:  0\n",
      "all_executions_overall:  263\n",
      "all_executions_accuracy:  263\n",
      "accurate_executions:  217\n",
      "failed_executions:  0\n",
      "all_executions_overall:  264\n",
      "all_executions_accuracy:  264\n",
      "accurate_executions:  218\n",
      "failed_executions:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_executions_overall:  265\n",
      "all_executions_accuracy:  265\n",
      "accurate_executions:  219\n",
      "failed_executions:  0\n",
      "all_executions_overall:  266\n",
      "all_executions_accuracy:  266\n",
      "accurate_executions:  220\n",
      "failed_executions:  0\n",
      "all_executions_overall:  267\n",
      "all_executions_accuracy:  267\n",
      "accurate_executions:  221\n",
      "failed_executions:  0\n",
      "all_executions_overall:  268\n",
      "all_executions_accuracy:  268\n",
      "accurate_executions:  222\n",
      "failed_executions:  0\n",
      "all_executions_overall:  269\n",
      "all_executions_accuracy:  269\n",
      "accurate_executions:  223\n",
      "failed_executions:  0\n",
      "all_executions_overall:  270\n",
      "all_executions_accuracy:  270\n",
      "accurate_executions:  224\n",
      "failed_executions:  0\n",
      "all_executions_overall:  271\n",
      "all_executions_accuracy:  271\n",
      "accurate_executions:  224\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  271\n",
      "all_executions_accuracy:  271\n",
      "accurate_executions:  224\n",
      "failed_executions:  0\n",
      "all_executions_overall:  272\n",
      "all_executions_accuracy:  272\n",
      "accurate_executions:  224\n",
      "failed_executions:  0\n",
      "all_executions_overall:  273\n",
      "all_executions_accuracy:  273\n",
      "accurate_executions:  225\n",
      "failed_executions:  0\n",
      "all_executions_overall:  274\n",
      "all_executions_accuracy:  274\n",
      "accurate_executions:  226\n",
      "failed_executions:  0\n",
      "all_executions_overall:  275\n",
      "all_executions_accuracy:  275\n",
      "accurate_executions:  227\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  275\n",
      "all_executions_accuracy:  275\n",
      "accurate_executions:  227\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  275\n",
      "all_executions_accuracy:  275\n",
      "accurate_executions:  227\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  275\n",
      "all_executions_accuracy:  275\n",
      "accurate_executions:  227\n",
      "failed_executions:  0\n",
      "all_executions_overall:  276\n",
      "all_executions_accuracy:  276\n",
      "accurate_executions:  228\n",
      "failed_executions:  0\n",
      "all_executions_overall:  277\n",
      "all_executions_accuracy:  277\n",
      "accurate_executions:  228\n",
      "failed_executions:  0\n",
      "all_executions_overall:  278\n",
      "all_executions_accuracy:  278\n",
      "accurate_executions:  228\n",
      "failed_executions:  0\n",
      "all_executions_overall:  279\n",
      "all_executions_accuracy:  279\n",
      "accurate_executions:  229\n",
      "failed_executions:  0\n",
      "all_executions_overall:  280\n",
      "all_executions_accuracy:  280\n",
      "accurate_executions:  230\n",
      "failed_executions:  0\n",
      "all_executions_overall:  281\n",
      "all_executions_accuracy:  281\n",
      "accurate_executions:  231\n",
      "failed_executions:  0\n",
      "all_executions_overall:  282\n",
      "all_executions_accuracy:  282\n",
      "accurate_executions:  232\n",
      "failed_executions:  0\n",
      "all_executions_overall:  283\n",
      "all_executions_accuracy:  283\n",
      "accurate_executions:  232\n",
      "failed_executions:  0\n",
      "all_executions_overall:  284\n",
      "all_executions_accuracy:  284\n",
      "accurate_executions:  233\n",
      "failed_executions:  0\n",
      "all_executions_overall:  285\n",
      "all_executions_accuracy:  285\n",
      "accurate_executions:  233\n",
      "failed_executions:  0\n",
      "all_executions_overall:  286\n",
      "all_executions_accuracy:  286\n",
      "accurate_executions:  234\n",
      "failed_executions:  0\n",
      "all_executions_overall:  287\n",
      "all_executions_accuracy:  287\n",
      "accurate_executions:  235\n",
      "failed_executions:  0\n",
      "all_executions_overall:  288\n",
      "all_executions_accuracy:  288\n",
      "accurate_executions:  236\n",
      "failed_executions:  0\n",
      "all_executions_overall:  289\n",
      "all_executions_accuracy:  289\n",
      "accurate_executions:  237\n",
      "failed_executions:  0\n",
      "all_executions_overall:  290\n",
      "all_executions_accuracy:  290\n",
      "accurate_executions:  238\n",
      "failed_executions:  0\n",
      "all_executions_overall:  291\n",
      "all_executions_accuracy:  291\n",
      "accurate_executions:  239\n",
      "failed_executions:  0\n",
      "all_executions_overall:  292\n",
      "all_executions_accuracy:  292\n",
      "accurate_executions:  240\n",
      "failed_executions:  0\n",
      "all_executions_overall:  293\n",
      "all_executions_accuracy:  293\n",
      "accurate_executions:  241\n",
      "failed_executions:  0\n",
      "all_executions_overall:  294\n",
      "all_executions_accuracy:  294\n",
      "accurate_executions:  242\n",
      "failed_executions:  0\n",
      "all_executions_overall:  295\n",
      "all_executions_accuracy:  295\n",
      "accurate_executions:  243\n",
      "failed_executions:  0\n",
      "all_executions_overall:  296\n",
      "all_executions_accuracy:  296\n",
      "accurate_executions:  244\n",
      "failed_executions:  0\n",
      "all_executions_overall:  297\n",
      "all_executions_accuracy:  297\n",
      "accurate_executions:  245\n",
      "failed_executions:  0\n",
      "all_executions_overall:  298\n",
      "all_executions_accuracy:  298\n",
      "accurate_executions:  246\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  298\n",
      "all_executions_accuracy:  298\n",
      "accurate_executions:  246\n",
      "failed_executions:  0\n",
      "all_executions_overall:  299\n",
      "all_executions_accuracy:  299\n",
      "accurate_executions:  247\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  299\n",
      "all_executions_accuracy:  299\n",
      "accurate_executions:  247\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  299\n",
      "all_executions_accuracy:  299\n",
      "accurate_executions:  247\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  299\n",
      "all_executions_accuracy:  299\n",
      "accurate_executions:  247\n",
      "failed_executions:  0\n",
      "all_executions_overall:  300\n",
      "all_executions_accuracy:  300\n",
      "accurate_executions:  248\n",
      "failed_executions:  0\n",
      "all_executions_overall:  301\n",
      "all_executions_accuracy:  301\n",
      "accurate_executions:  249\n",
      "failed_executions:  0\n",
      "all_executions_overall:  302\n",
      "all_executions_accuracy:  302\n",
      "accurate_executions:  250\n",
      "failed_executions:  0\n",
      "all_executions_overall:  303\n",
      "all_executions_accuracy:  303\n",
      "accurate_executions:  251\n",
      "failed_executions:  0\n",
      "all_executions_overall:  304\n",
      "all_executions_accuracy:  304\n",
      "accurate_executions:  252\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  304\n",
      "all_executions_accuracy:  304\n",
      "accurate_executions:  252\n",
      "failed_executions:  0\n",
      "all_executions_overall:  305\n",
      "all_executions_accuracy:  305\n",
      "accurate_executions:  253\n",
      "failed_executions:  0\n",
      "all_executions_overall:  306\n",
      "all_executions_accuracy:  306\n",
      "accurate_executions:  253\n",
      "failed_executions:  0\n",
      "all_executions_overall:  307\n",
      "all_executions_accuracy:  307\n",
      "accurate_executions:  254\n",
      "failed_executions:  0\n",
      "all_executions_overall:  308\n",
      "all_executions_accuracy:  308\n",
      "accurate_executions:  255\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  308\n",
      "all_executions_accuracy:  308\n",
      "accurate_executions:  255\n",
      "failed_executions:  0\n",
      "all_executions_overall:  309\n",
      "all_executions_accuracy:  309\n",
      "accurate_executions:  256\n",
      "failed_executions:  0\n",
      "all_executions_overall:  310\n",
      "all_executions_accuracy:  310\n",
      "accurate_executions:  257\n",
      "failed_executions:  0\n",
      "all_executions_overall:  311\n",
      "all_executions_accuracy:  311\n",
      "accurate_executions:  258\n",
      "failed_executions:  0\n",
      "all_executions_overall:  312\n",
      "all_executions_accuracy:  312\n",
      "accurate_executions:  259\n",
      "failed_executions:  0\n",
      "all_executions_overall:  313\n",
      "all_executions_accuracy:  313\n",
      "accurate_executions:  259\n",
      "failed_executions:  0\n",
      "all_executions_overall:  314\n",
      "all_executions_accuracy:  314\n",
      "accurate_executions:  260\n",
      "failed_executions:  0\n",
      "all_executions_overall:  315\n",
      "all_executions_accuracy:  315\n",
      "accurate_executions:  261\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  315\n",
      "all_executions_accuracy:  315\n",
      "accurate_executions:  261\n",
      "failed_executions:  0\n",
      "all_executions_overall:  316\n",
      "all_executions_accuracy:  316\n",
      "accurate_executions:  261\n",
      "failed_executions:  0\n",
      "all_executions_overall:  317\n",
      "all_executions_accuracy:  317\n",
      "accurate_executions:  262\n",
      "failed_executions:  0\n",
      "all_executions_overall:  318\n",
      "all_executions_accuracy:  318\n",
      "accurate_executions:  263\n",
      "failed_executions:  0\n",
      "all_executions_overall:  319\n",
      "all_executions_accuracy:  319\n",
      "accurate_executions:  263\n",
      "failed_executions:  0\n",
      "all_executions_overall:  320\n",
      "all_executions_accuracy:  320\n",
      "accurate_executions:  263\n",
      "failed_executions:  0\n",
      "all_executions_overall:  321\n",
      "all_executions_accuracy:  321\n",
      "accurate_executions:  264\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  321\n",
      "all_executions_accuracy:  321\n",
      "accurate_executions:  264\n",
      "failed_executions:  0\n",
      "all_executions_overall:  322\n",
      "all_executions_accuracy:  322\n",
      "accurate_executions:  265\n",
      "failed_executions:  0\n",
      "all_executions_overall:  323\n",
      "all_executions_accuracy:  323\n",
      "accurate_executions:  266\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  323\n",
      "all_executions_accuracy:  323\n",
      "accurate_executions:  266\n",
      "failed_executions:  0\n",
      "all_executions_overall:  324\n",
      "all_executions_accuracy:  324\n",
      "accurate_executions:  267\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  324\n",
      "all_executions_accuracy:  324\n",
      "accurate_executions:  267\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  324\n",
      "all_executions_accuracy:  324\n",
      "accurate_executions:  267\n",
      "failed_executions:  0\n",
      "all_executions_overall:  325\n",
      "all_executions_accuracy:  325\n",
      "accurate_executions:  268\n",
      "failed_executions:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_executions_overall:  326\n",
      "all_executions_accuracy:  326\n",
      "accurate_executions:  268\n",
      "failed_executions:  0\n",
      "all_executions_overall:  327\n",
      "all_executions_accuracy:  327\n",
      "accurate_executions:  269\n",
      "failed_executions:  0\n",
      "all_executions_overall:  328\n",
      "all_executions_accuracy:  328\n",
      "accurate_executions:  270\n",
      "failed_executions:  0\n",
      "all_executions_overall:  329\n",
      "all_executions_accuracy:  329\n",
      "accurate_executions:  270\n",
      "failed_executions:  0\n",
      "all_executions_overall:  330\n",
      "all_executions_accuracy:  330\n",
      "accurate_executions:  271\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  330\n",
      "all_executions_accuracy:  330\n",
      "accurate_executions:  271\n",
      "failed_executions:  0\n",
      "all_executions_overall:  331\n",
      "all_executions_accuracy:  331\n",
      "accurate_executions:  271\n",
      "failed_executions:  0\n",
      "all_executions_overall:  332\n",
      "all_executions_accuracy:  332\n",
      "accurate_executions:  272\n",
      "failed_executions:  0\n",
      "all_executions_overall:  333\n",
      "all_executions_accuracy:  333\n",
      "accurate_executions:  273\n",
      "failed_executions:  0\n",
      "all_executions_overall:  334\n",
      "all_executions_accuracy:  334\n",
      "accurate_executions:  274\n",
      "failed_executions:  0\n",
      "all_executions_overall:  335\n",
      "all_executions_accuracy:  335\n",
      "accurate_executions:  275\n",
      "failed_executions:  0\n",
      "all_executions_overall:  336\n",
      "all_executions_accuracy:  336\n",
      "accurate_executions:  276\n",
      "failed_executions:  0\n",
      "all_executions_overall:  337\n",
      "all_executions_accuracy:  337\n",
      "accurate_executions:  277\n",
      "failed_executions:  0\n",
      "all_executions_overall:  338\n",
      "all_executions_accuracy:  338\n",
      "accurate_executions:  278\n",
      "failed_executions:  0\n",
      "all_executions_overall:  339\n",
      "all_executions_accuracy:  339\n",
      "accurate_executions:  279\n",
      "failed_executions:  0\n",
      "all_executions_overall:  340\n",
      "all_executions_accuracy:  340\n",
      "accurate_executions:  280\n",
      "failed_executions:  0\n",
      "all_executions_overall:  341\n",
      "all_executions_accuracy:  341\n",
      "accurate_executions:  281\n",
      "failed_executions:  0\n",
      "all_executions_overall:  342\n",
      "all_executions_accuracy:  342\n",
      "accurate_executions:  281\n",
      "failed_executions:  0\n",
      "all_executions_overall:  343\n",
      "all_executions_accuracy:  343\n",
      "accurate_executions:  282\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  343\n",
      "all_executions_accuracy:  343\n",
      "accurate_executions:  282\n",
      "failed_executions:  0\n",
      "all_executions_overall:  344\n",
      "all_executions_accuracy:  344\n",
      "accurate_executions:  283\n",
      "failed_executions:  0\n",
      "all_executions_overall:  345\n",
      "all_executions_accuracy:  345\n",
      "accurate_executions:  284\n",
      "failed_executions:  0\n",
      "all_executions_overall:  346\n",
      "all_executions_accuracy:  346\n",
      "accurate_executions:  284\n",
      "failed_executions:  0\n",
      "all_executions_overall:  347\n",
      "all_executions_accuracy:  347\n",
      "accurate_executions:  285\n",
      "failed_executions:  0\n",
      "all_executions_overall:  348\n",
      "all_executions_accuracy:  348\n",
      "accurate_executions:  286\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  348\n",
      "all_executions_accuracy:  348\n",
      "accurate_executions:  286\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  348\n",
      "all_executions_accuracy:  348\n",
      "accurate_executions:  286\n",
      "failed_executions:  0\n",
      "all_executions_overall:  349\n",
      "all_executions_accuracy:  349\n",
      "accurate_executions:  286\n",
      "failed_executions:  0\n",
      "all_executions_overall:  350\n",
      "all_executions_accuracy:  350\n",
      "accurate_executions:  287\n",
      "failed_executions:  0\n",
      "all_executions_overall:  351\n",
      "all_executions_accuracy:  351\n",
      "accurate_executions:  287\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  351\n",
      "all_executions_accuracy:  351\n",
      "accurate_executions:  287\n",
      "failed_executions:  0\n",
      "all_executions_overall:  352\n",
      "all_executions_accuracy:  352\n",
      "accurate_executions:  288\n",
      "failed_executions:  0\n",
      "all_executions_overall:  353\n",
      "all_executions_accuracy:  353\n",
      "accurate_executions:  289\n",
      "failed_executions:  0\n",
      "all_executions_overall:  354\n",
      "all_executions_accuracy:  354\n",
      "accurate_executions:  290\n",
      "failed_executions:  0\n",
      "all_executions_overall:  355\n",
      "all_executions_accuracy:  355\n",
      "accurate_executions:  291\n",
      "failed_executions:  0\n",
      "all_executions_overall:  356\n",
      "all_executions_accuracy:  356\n",
      "accurate_executions:  292\n",
      "failed_executions:  0\n",
      "all_executions_overall:  357\n",
      "all_executions_accuracy:  357\n",
      "accurate_executions:  292\n",
      "failed_executions:  0\n",
      "all_executions_overall:  358\n",
      "all_executions_accuracy:  358\n",
      "accurate_executions:  293\n",
      "failed_executions:  0\n",
      "all_executions_overall:  359\n",
      "all_executions_accuracy:  359\n",
      "accurate_executions:  294\n",
      "failed_executions:  0\n",
      "all_executions_overall:  360\n",
      "all_executions_accuracy:  360\n",
      "accurate_executions:  295\n",
      "failed_executions:  0\n",
      "all_executions_overall:  361\n",
      "all_executions_accuracy:  361\n",
      "accurate_executions:  296\n",
      "failed_executions:  0\n",
      "all_executions_overall:  362\n",
      "all_executions_accuracy:  362\n",
      "accurate_executions:  297\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  362\n",
      "all_executions_accuracy:  362\n",
      "accurate_executions:  297\n",
      "failed_executions:  0\n",
      "all_executions_overall:  363\n",
      "all_executions_accuracy:  363\n",
      "accurate_executions:  298\n",
      "failed_executions:  0\n",
      "all_executions_overall:  364\n",
      "all_executions_accuracy:  364\n",
      "accurate_executions:  299\n",
      "failed_executions:  0\n",
      "all_executions_overall:  365\n",
      "all_executions_accuracy:  365\n",
      "accurate_executions:  300\n",
      "failed_executions:  0\n",
      "all_executions_overall:  366\n",
      "all_executions_accuracy:  366\n",
      "accurate_executions:  301\n",
      "failed_executions:  0\n",
      "all_executions_overall:  367\n",
      "all_executions_accuracy:  367\n",
      "accurate_executions:  302\n",
      "failed_executions:  0\n",
      "all_executions_overall:  368\n",
      "all_executions_accuracy:  368\n",
      "accurate_executions:  302\n",
      "failed_executions:  0\n",
      "all_executions_overall:  369\n",
      "all_executions_accuracy:  369\n",
      "accurate_executions:  303\n",
      "failed_executions:  0\n",
      "all_executions_overall:  370\n",
      "all_executions_accuracy:  370\n",
      "accurate_executions:  304\n",
      "failed_executions:  0\n",
      "all_executions_overall:  371\n",
      "all_executions_accuracy:  371\n",
      "accurate_executions:  305\n",
      "failed_executions:  0\n",
      "all_executions_overall:  372\n",
      "all_executions_accuracy:  372\n",
      "accurate_executions:  306\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  372\n",
      "all_executions_accuracy:  372\n",
      "accurate_executions:  306\n",
      "failed_executions:  0\n",
      "all_executions_overall:  373\n",
      "all_executions_accuracy:  373\n",
      "accurate_executions:  307\n",
      "failed_executions:  0\n",
      "all_executions_overall:  374\n",
      "all_executions_accuracy:  374\n",
      "accurate_executions:  308\n",
      "failed_executions:  0\n",
      "all_executions_overall:  375\n",
      "all_executions_accuracy:  375\n",
      "accurate_executions:  309\n",
      "failed_executions:  0\n",
      "all_executions_overall:  376\n",
      "all_executions_accuracy:  376\n",
      "accurate_executions:  310\n",
      "failed_executions:  0\n",
      "all_executions_overall:  377\n",
      "all_executions_accuracy:  377\n",
      "accurate_executions:  310\n",
      "failed_executions:  0\n",
      "all_executions_overall:  378\n",
      "all_executions_accuracy:  378\n",
      "accurate_executions:  310\n",
      "failed_executions:  0\n",
      "all_executions_overall:  379\n",
      "all_executions_accuracy:  379\n",
      "accurate_executions:  311\n",
      "failed_executions:  0\n",
      "all_executions_overall:  380\n",
      "all_executions_accuracy:  380\n",
      "accurate_executions:  312\n",
      "failed_executions:  0\n",
      "all_executions_overall:  381\n",
      "all_executions_accuracy:  381\n",
      "accurate_executions:  313\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  381\n",
      "all_executions_accuracy:  381\n",
      "accurate_executions:  313\n",
      "failed_executions:  0\n",
      "all_executions_overall:  382\n",
      "all_executions_accuracy:  382\n",
      "accurate_executions:  313\n",
      "failed_executions:  0\n",
      "all_executions_overall:  383\n",
      "all_executions_accuracy:  383\n",
      "accurate_executions:  313\n",
      "failed_executions:  0\n",
      "all_executions_overall:  384\n",
      "all_executions_accuracy:  384\n",
      "accurate_executions:  314\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  384\n",
      "all_executions_accuracy:  384\n",
      "accurate_executions:  314\n",
      "failed_executions:  0\n",
      "all_executions_overall:  385\n",
      "all_executions_accuracy:  385\n",
      "accurate_executions:  315\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  385\n",
      "all_executions_accuracy:  385\n",
      "accurate_executions:  315\n",
      "failed_executions:  0\n",
      "all_executions_overall:  386\n",
      "all_executions_accuracy:  386\n",
      "accurate_executions:  316\n",
      "failed_executions:  0\n",
      "all_executions_overall:  387\n",
      "all_executions_accuracy:  387\n",
      "accurate_executions:  316\n",
      "failed_executions:  0\n",
      "all_executions_overall:  388\n",
      "all_executions_accuracy:  388\n",
      "accurate_executions:  316\n",
      "failed_executions:  0\n",
      "all_executions_overall:  389\n",
      "all_executions_accuracy:  389\n",
      "accurate_executions:  316\n",
      "failed_executions:  0\n",
      "all_executions_overall:  390\n",
      "all_executions_accuracy:  390\n",
      "accurate_executions:  317\n",
      "failed_executions:  0\n",
      "all_executions_overall:  391\n",
      "all_executions_accuracy:  391\n",
      "accurate_executions:  318\n",
      "failed_executions:  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_executions_overall:  392\n",
      "all_executions_accuracy:  392\n",
      "accurate_executions:  318\n",
      "failed_executions:  0\n",
      "all_executions_overall:  393\n",
      "all_executions_accuracy:  393\n",
      "accurate_executions:  318\n",
      "failed_executions:  0\n",
      "all_executions_overall:  394\n",
      "all_executions_accuracy:  394\n",
      "accurate_executions:  319\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  394\n",
      "all_executions_accuracy:  394\n",
      "accurate_executions:  319\n",
      "failed_executions:  0\n",
      "all_executions_overall:  395\n",
      "all_executions_accuracy:  395\n",
      "accurate_executions:  320\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  395\n",
      "all_executions_accuracy:  395\n",
      "accurate_executions:  320\n",
      "failed_executions:  0\n",
      "all_executions_overall:  396\n",
      "all_executions_accuracy:  396\n",
      "accurate_executions:  320\n",
      "failed_executions:  0\n",
      "all_executions_overall:  397\n",
      "all_executions_accuracy:  397\n",
      "accurate_executions:  321\n",
      "failed_executions:  0\n",
      "all_executions_overall:  398\n",
      "all_executions_accuracy:  398\n",
      "accurate_executions:  322\n",
      "failed_executions:  0\n",
      "all_executions_overall:  399\n",
      "all_executions_accuracy:  399\n",
      "accurate_executions:  323\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  399\n",
      "all_executions_accuracy:  399\n",
      "accurate_executions:  323\n",
      "failed_executions:  0\n",
      "all_executions_overall:  400\n",
      "all_executions_accuracy:  400\n",
      "accurate_executions:  324\n",
      "failed_executions:  0\n",
      "all_executions_overall:  401\n",
      "all_executions_accuracy:  401\n",
      "accurate_executions:  325\n",
      "failed_executions:  0\n",
      "all_executions_overall:  402\n",
      "all_executions_accuracy:  402\n",
      "accurate_executions:  326\n",
      "failed_executions:  0\n",
      "all_executions_overall:  403\n",
      "all_executions_accuracy:  403\n",
      "accurate_executions:  327\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  403\n",
      "all_executions_accuracy:  403\n",
      "accurate_executions:  327\n",
      "failed_executions:  0\n",
      "all_executions_overall:  404\n",
      "all_executions_accuracy:  404\n",
      "accurate_executions:  328\n",
      "failed_executions:  0\n",
      "all_executions_overall:  405\n",
      "all_executions_accuracy:  405\n",
      "accurate_executions:  329\n",
      "failed_executions:  0\n",
      "all_executions_overall:  406\n",
      "all_executions_accuracy:  406\n",
      "accurate_executions:  329\n",
      "failed_executions:  0\n",
      "all_executions_overall:  407\n",
      "all_executions_accuracy:  407\n",
      "accurate_executions:  329\n",
      "failed_executions:  0\n",
      "all_executions_overall:  408\n",
      "all_executions_accuracy:  408\n",
      "accurate_executions:  330\n",
      "failed_executions:  0\n",
      "all_executions_overall:  409\n",
      "all_executions_accuracy:  409\n",
      "accurate_executions:  331\n",
      "failed_executions:  0\n",
      "all_executions_overall:  410\n",
      "all_executions_accuracy:  410\n",
      "accurate_executions:  331\n",
      "failed_executions:  0\n",
      "all_executions_overall:  411\n",
      "all_executions_accuracy:  411\n",
      "accurate_executions:  331\n",
      "failed_executions:  0\n",
      "all_executions_overall:  412\n",
      "all_executions_accuracy:  412\n",
      "accurate_executions:  332\n",
      "failed_executions:  0\n",
      "all_executions_overall:  413\n",
      "all_executions_accuracy:  413\n",
      "accurate_executions:  333\n",
      "failed_executions:  0\n",
      "all_executions_overall:  414\n",
      "all_executions_accuracy:  414\n",
      "accurate_executions:  334\n",
      "failed_executions:  0\n",
      "all_executions_overall:  415\n",
      "all_executions_accuracy:  415\n",
      "accurate_executions:  335\n",
      "failed_executions:  0\n",
      "all_executions_overall:  416\n",
      "all_executions_accuracy:  416\n",
      "accurate_executions:  335\n",
      "failed_executions:  0\n",
      "all_executions_overall:  417\n",
      "all_executions_accuracy:  417\n",
      "accurate_executions:  336\n",
      "failed_executions:  0\n",
      "all_executions_overall:  418\n",
      "all_executions_accuracy:  418\n",
      "accurate_executions:  337\n",
      "failed_executions:  0\n",
      "all_executions_overall:  419\n",
      "all_executions_accuracy:  419\n",
      "accurate_executions:  338\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  419\n",
      "all_executions_accuracy:  419\n",
      "accurate_executions:  338\n",
      "failed_executions:  0\n",
      "all_executions_overall:  420\n",
      "all_executions_accuracy:  420\n",
      "accurate_executions:  338\n",
      "failed_executions:  0\n",
      "all_executions_overall:  421\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  0\n",
      "except_1\n",
      "all_executions_overall:  421\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  0\n",
      "except_2\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n",
      "except_1\n",
      "all_executions_overall:  422\n",
      "all_executions_accuracy:  421\n",
      "accurate_executions:  338\n",
      "failed_executions:  1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m failed_predicted_SQL \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m tokenized_dataset:\n\u001b[0;32m---> 38\u001b[0m     p,l \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_executions_overall: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_executions_overall))\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_executions_accuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;28mlen\u001b[39m(all_executions_accuracy))\n",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m, in \u001b[0;36mevaluate_peft_model\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_peft_model\u001b[39m(sample):\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m     label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, sample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m], tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/peft/peft_model.py:1118\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(peft_config, PromptLearningConfig):\n\u001b[0;32m-> 1118\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1533\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgreedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1552\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/generation/utils.py:2362\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2362\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2370\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1717\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1714\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1717\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1732\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1094\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1081\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1082\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1083\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1092\u001b[0m     )\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1094\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:694\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    704\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:601\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    592\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    599\u001b[0m ):\n\u001b[1;32m    600\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 601\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    611\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:526\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# get key/value states\u001b[39;00m\n\u001b[1;32m    523\u001b[0m key_states \u001b[38;5;241m=\u001b[39m project(\n\u001b[1;32m    524\u001b[0m     hidden_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk, key_value_states, past_key_value[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    525\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    528\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;66;03m# compute scores\u001b[39;00m\n\u001b[1;32m    531\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[1;32m    532\u001b[0m     query_states, key_states\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    533\u001b[0m )  \u001b[38;5;66;03m# equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:497\u001b[0m, in \u001b[0;36mT5Attention.forward.<locals>.project\u001b[0;34m(hidden_states, proj_layer, key_value_states, past_key_value)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"projects hidden states correctly to key/query states\"\"\"\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key_value_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# self-attn\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(\u001b[43mproj_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# cross-attn\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m shape(proj_layer(key_value_states))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/peft/tuners/lora.py:703\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    698\u001b[0m     result \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(x, transpose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfan_in_fan_out), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    700\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_A[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapter]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    702\u001b[0m     result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 703\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_B\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_A\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_dropout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_adapter\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    707\u001b[0m     )\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     result \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(x, transpose(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfan_in_fan_out), bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "\n",
    "connection = mysql.connector.connect(\n",
    "    host=\"relational.fit.cvut.cz\",\n",
    "    user=\"guest\",\n",
    "    password=\"relational\",\n",
    "    database=\"imdb_ijs\"\n",
    ")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "\n",
    "\n",
    "print(\"mapping both datasets\")\n",
    "tokenized_dataset = test_set_seen.map(convert_to_features, batched=True, num_proc=4)\n",
    "tokenized_dataset_2 = test_set_unseen.map(convert_to_features, batched=True, num_proc=4)\n",
    "print(\"mapped both dataset\")\n",
    "print(\"Documents we have: tokenizer_dataset for seen and tokenized_dataset_2 for unseen data\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n Running executions for seen dataset\")\n",
    "all_executions_overall = []\n",
    "failed_executions = []\n",
    "all_executions_accuracy = []\n",
    "accurate_executions = []\n",
    "for_checking_label = []\n",
    "for_checking_prediction = []\n",
    "failed_original_SQL = []\n",
    "failed_predicted_SQL = []\n",
    "\n",
    "\n",
    "\n",
    "for sample in tokenized_dataset:\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "    print(\"all_executions_overall: \", len(all_executions_overall))\n",
    "    print(\"all_executions_accuracy: \",len(all_executions_accuracy))\n",
    "    print(\"accurate_executions: \", len(accurate_executions))\n",
    "    print(\"failed_executions: \", len(failed_executions))\n",
    "\n",
    "\n",
    "print(\"All SQL runs:\", len(all_executions_overall))\n",
    "print(\"Model SQLs that failed: \" len(failed_executions))\n",
    "print(f\"Execution rate: {len(all_executions_accuracy)/len(all_executions_overall)*100}%\")\n",
    "print(f\"Execution rate: {100 - len(failed_executions)/len(all_executions_overall)*100}%\")\n",
    "print(f\"Execution accuracy: {len(accurate_executions)/len(all_executions_accuracy)*100}%\")\n",
    "\n",
    "failed_original_sql_df = pd.DataFrame(failed_original_SQL)\n",
    "failed_predicted_sql_df = pd.DataFrame(failed_predicted_SQL)\n",
    "not_equals = pd.DataFrame({\n",
    "    'Label':for_checking_label,\n",
    "    'Prediction': for_checking_prediction\n",
    "})\n",
    "\n",
    "\n",
    "not_equals.to_csv(\"/home/toibazd/Data/Text2SQL/Training_set_IMDB/Not_equals_lora.csv\", index = False)\n",
    "failed_original_sql_df.to_csv(\"/home/toibazd/Data/Text2SQL/Training_set_IMDB/Failed_originals_lora.csv\", index = False)\n",
    "failed_predicted_sql_df.to_csv(\"/home/toibazd/Data/Text2SQL/Training_set_IMDB/Failed_predicted_lora.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d89262",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n \\n Running evaluation on unseen data\")\n",
    "all_executions_overall = []\n",
    "failed_executions = []\n",
    "all_executions_accuracy = []\n",
    "accurate_executions = []\n",
    "for_checking_label = []\n",
    "for_checking_prediction = []\n",
    "failed_original_SQL = []\n",
    "failed_predicted_SQL = []\n",
    "\n",
    "for sample in tokenized_dataset_2:\n",
    "    p,l = evaluate_peft_model(sample)\n",
    "\n",
    "print(\"All SQL runs:\", len(all_executions_overall))\n",
    "print(\"Model SQLs that failed: \" len(failed_executions))\n",
    "print(f\"Execution rate: {len(all_executions_accuracy)/len(all_executions_overall)*100}%\")\n",
    "print(f\"Execution rate: {100 - len(failed_executions)/len(all_executions_overall)*100}%\")\n",
    "print(f\"Execution accuracy: {len(accurate_executions)/len(all_executions_accuracy)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4076606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0a031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641c0dee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
